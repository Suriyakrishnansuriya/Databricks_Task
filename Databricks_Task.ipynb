{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc67c47-1394-46f7-ab4f-0b3aaa01b4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (1.4.4)\r\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas) (1.21.5)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2022.1)\r\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.2\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b39109cb-4089-4a42-b45a-b30e8df4936a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Min_of_volume</th>\n",
       "      <th>Max_of_volume</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80898100</td>\n",
       "      <td>2.094900e+06</td>\n",
       "      <td>61.060001</td>\n",
       "      <td>59.869999</td>\n",
       "      <td>60.330002</td>\n",
       "      <td>12668800</td>\n",
       "      <td>52.619235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>6.021000e+01</td>\n",
       "      <td>60.349998</td>\n",
       "      <td>59.470001</td>\n",
       "      <td>59.709999</td>\n",
       "      <td>9593300</td>\n",
       "      <td>52.078475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>5.935000e+01</td>\n",
       "      <td>59.619999</td>\n",
       "      <td>58.369999</td>\n",
       "      <td>59.419998</td>\n",
       "      <td>12768200</td>\n",
       "      <td>51.825539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>5.942000e+01</td>\n",
       "      <td>59.450001</td>\n",
       "      <td>58.869999</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>8069400</td>\n",
       "      <td>51.459220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>5.903000e+01</td>\n",
       "      <td>59.549999</td>\n",
       "      <td>58.919998</td>\n",
       "      <td>59.180000</td>\n",
       "      <td>6679300</td>\n",
       "      <td>51.616215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>6.943000e+01</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>69.360001</td>\n",
       "      <td>69.540001</td>\n",
       "      <td>4803900</td>\n",
       "      <td>69.032411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2016-12-27</td>\n",
       "      <td>6.930000e+01</td>\n",
       "      <td>69.820000</td>\n",
       "      <td>69.250000</td>\n",
       "      <td>69.699997</td>\n",
       "      <td>4435700</td>\n",
       "      <td>69.191240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2016-12-28</td>\n",
       "      <td>6.994000e+01</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>69.260002</td>\n",
       "      <td>69.309998</td>\n",
       "      <td>4875700</td>\n",
       "      <td>68.804087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>6.921000e+01</td>\n",
       "      <td>69.519997</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>69.260002</td>\n",
       "      <td>4298400</td>\n",
       "      <td>68.754456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>6.912000e+01</td>\n",
       "      <td>69.430000</td>\n",
       "      <td>68.830002</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>6889500</td>\n",
       "      <td>68.615479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Min_of_volume  Max_of_volume  ...      Close    Volume  Adj Close\n",
       "0       80898100   2.094900e+06      61.060001  ...  60.330002  12668800  52.619235\n",
       "1     2012-01-04   6.021000e+01      60.349998  ...  59.709999   9593300  52.078475\n",
       "2     2012-01-05   5.935000e+01      59.619999  ...  59.419998  12768200  51.825539\n",
       "3     2012-01-06   5.942000e+01      59.450001  ...  59.000000   8069400  51.459220\n",
       "4     2012-01-09   5.903000e+01      59.549999  ...  59.180000   6679300  51.616215\n",
       "...          ...            ...            ...  ...        ...       ...        ...\n",
       "1253  2016-12-23   6.943000e+01      69.750000  ...  69.540001   4803900  69.032411\n",
       "1254  2016-12-27   6.930000e+01      69.820000  ...  69.699997   4435700  69.191240\n",
       "1255  2016-12-28   6.994000e+01      70.000000  ...  69.309998   4875700  68.804087\n",
       "1256  2016-12-29   6.921000e+01      69.519997  ...  69.260002   4298400  68.754456\n",
       "1257  2016-12-30   6.912000e+01      69.430000  ...  69.120003   6889500  68.615479\n",
       "\n",
       "[1258 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing important modules\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = pd.read_csv(\"/dbfs/FileStore/shared_uploads/suriyakrishnan4798@gmail.com/walmart_stock.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a22bd7-c9a1-485f-bb55-dcc2691049d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1655643824988890>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Assuming df is the dataframe\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# scenario 1:  print out first 5 columns\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m[:, :\u001B[38;5;241m5\u001B[39m])\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3068\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   3035\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   3036\u001B[0m \n",
       "\u001B[1;32m   3037\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3065\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   3066\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 3068\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n",
       "\u001B[1;32m   3069\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n",
       "\u001B[1;32m   3070\u001B[0m     )\n",
       "\u001B[1;32m   3071\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   3072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'iloc'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1655643824988890>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Assuming df is the dataframe\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# scenario 1:  print out first 5 columns\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m[:, :\u001B[38;5;241m5\u001B[39m])\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3068\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   3035\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   3036\u001B[0m \n\u001B[1;32m   3037\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3065\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   3066\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 3068\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   3069\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   3070\u001B[0m     )\n\u001B[1;32m   3071\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   3072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'iloc'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'iloc'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming df is the dataframe\n",
    "# scenario 1:  print out first 5 columns\n",
    "print(df1.iloc[:, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8473db-18be-412d-89c0-287680ab1b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1655643824988891>, line 9\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''scenario2:There are too many decimal places for mean \u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mand stddev in the describe() dataframe. Format the\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m numbers to just show up to two decimal places. \u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03mbut we covered something very similar. \u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03mCheck this link for a hintÂ¶'''\u001B[39;00m\n",
       "\u001B[0;32m----> 9\u001B[0m df1\u001B[38;5;241m.\u001B[39mdescribe()\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3068\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   3035\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   3036\u001B[0m \n",
       "\u001B[1;32m   3037\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3065\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   3066\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 3068\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n",
       "\u001B[1;32m   3069\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n",
       "\u001B[1;32m   3070\u001B[0m     )\n",
       "\u001B[1;32m   3071\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   3072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'round'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1655643824988891>, line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m'''scenario2:There are too many decimal places for mean \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03mand stddev in the describe() dataframe. Format the\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m numbers to just show up to two decimal places. \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03mbut we covered something very similar. \u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03mCheck this link for a hintÂ¶'''\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m df1\u001B[38;5;241m.\u001B[39mdescribe()\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3068\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   3035\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   3036\u001B[0m \n\u001B[1;32m   3037\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3065\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   3066\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 3068\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   3069\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   3070\u001B[0m     )\n\u001B[1;32m   3071\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   3072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'round'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'round'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''scenario2:There are too many decimal places for mean \n",
    "and stddev in the describe() dataframe. Format the\n",
    " numbers to just show up to two decimal places. \n",
    "Pay careful attention to the datatypes that .describe() \n",
    "returns, we didn't cover how to do this exact formatting, \n",
    "but we covered something very similar. \n",
    "Check this link for a hintÂ¶'''\n",
    "\n",
    "df1.describe().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c02a8a2-3494-4776-9338-0661e740d136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Min_of_volume</th>\n",
       "      <th>Max_of_volume</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80898100</td>\n",
       "      <td>2.094900e+06</td>\n",
       "      <td>61.060001</td>\n",
       "      <td>59.869999</td>\n",
       "      <td>60.330002</td>\n",
       "      <td>12668800</td>\n",
       "      <td>52.619235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>6.021000e+01</td>\n",
       "      <td>60.349998</td>\n",
       "      <td>59.470001</td>\n",
       "      <td>59.709999</td>\n",
       "      <td>9593300</td>\n",
       "      <td>52.078475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>5.935000e+01</td>\n",
       "      <td>59.619999</td>\n",
       "      <td>58.369999</td>\n",
       "      <td>59.419998</td>\n",
       "      <td>12768200</td>\n",
       "      <td>51.825539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>5.942000e+01</td>\n",
       "      <td>59.450001</td>\n",
       "      <td>58.869999</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>8069400</td>\n",
       "      <td>51.459220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>5.903000e+01</td>\n",
       "      <td>59.549999</td>\n",
       "      <td>58.919998</td>\n",
       "      <td>59.180000</td>\n",
       "      <td>6679300</td>\n",
       "      <td>51.616215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Min_of_volume  Max_of_volume  ...      Close    Volume  Adj Close\n",
       "0    80898100   2.094900e+06      61.060001  ...  60.330002  12668800  52.619235\n",
       "1  2012-01-04   6.021000e+01      60.349998  ...  59.709999   9593300  52.078475\n",
       "2  2012-01-05   5.935000e+01      59.619999  ...  59.419998  12768200  51.825539\n",
       "3  2012-01-06   5.942000e+01      59.450001  ...  59.000000   8069400  51.459220\n",
       "4  2012-01-09   5.903000e+01      59.549999  ...  59.180000   6679300  51.616215\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print head of the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818245b5-3311-4916-b3a4-e220b05cbcb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         80898100\n1       2012-01-04\n2       2012-01-05\n3       2012-01-06\n4       2012-01-09\n           ...    \n1253    2016-12-23\n1254    2016-12-27\n1255    2016-12-28\n1256    2016-12-29\n1257    2016-12-30\nName: Date, Length: 1258, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# rename collumn \n",
    "df.rename(columns= {'Unnamed: 0' : 'Date'}, inplace=True)\n",
    "print(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7046a037-546e-40a2-bca6-7f01670a182c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0         80898100\n",
       "1       2012-01-04\n",
       "2       2012-01-05\n",
       "3       2012-01-06\n",
       "4       2012-01-09\n",
       "           ...    \n",
       "1253    2016-12-23\n",
       "1254    2016-12-27\n",
       "1255    2016-12-28\n",
       "1256    2016-12-29\n",
       "1257    2016-12-30\n",
       "Name: Date, Length: 1258, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0bc7c93-5396-4f18-8ee4-35b4b8379534",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day with Peak High:             Date  Min_of_volume  Max_of_volume  ...      Close    Volume  Adj Close\n951  2015-10-14      66.610001      67.949997  ...  60.029999  80898100  57.429177\n\n[1 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# What day had the Peak High in Price\n",
    "peak_high_day = df[df['Volume'] == df['Volume'].max()]\n",
    "print(\"Day with Peak High: \", peak_high_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c8cbe3-65aa-4fb6-abd3-97eb2f98dd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Min_of_volume</th><th>Max_of_volume</th><th>Low</th><th>Close</th><th>Volume</th><th>Adj Close</th></tr></thead><tbody><tr><td>66.610001</td><td>67.949997</td><td>60.02</td><td>60.029999</td><td>80898100</td><td>57.429177</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         66.610001,
         67.949997,
         60.02,
         60.029999,
         80898100,
         57.429177
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 48
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Min_of_volume",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Max_of_volume",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Low",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Close",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Volume",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Adj Close",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- What is the max of the Volume column\n",
    "\n",
    "SELECT * FROM walmart_stock WHERE walmart_stock.Volume = (SELECT MAX(walmart_stock.Volume) FROM walmart_stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8d0582-fbe2-4ea5-86d8-8a2379260c88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Max_Volume</th><th>Min_Volume</th></tr></thead><tbody><tr><td>80898100</td><td>2094900</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         80898100,
         2094900
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 49
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Max_Volume",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Min_Volume",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--What is the max and min of the Volume column\n",
    "SELECT\n",
    "  MAX(Volume) AS Max_Volume,\n",
    "  MIN(Volume) AS Min_Volume\n",
    "FROM\n",
    "  walmart_stock;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa0c400-2441-440c-b51b-7ce3552c4e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Num_Days_Lower_Than_60</th></tr></thead><tbody><tr><td>81</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         81
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 50
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Num_Days_Lower_Than_60",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "-- How many days was the Close lower than 60 dollars\n",
    "SELECT\n",
    "  COUNT(*) AS Num_Days_Lower_Than_60\n",
    "FROM\n",
    "  walmart_stock\n",
    "WHERE\n",
    "  Close < 60;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415c5630-0059-4e60-91cb-0aa04cf2f8d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Num_Days_Higher_Than_80</th></tr></thead><tbody><tr><td>1148</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1148
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 51
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Num_Days_Higher_Than_80",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- What percentage of the time was the High greater than 80 dollars\n",
    "\n",
    "SELECT\n",
    "  COUNT(*) AS Num_Days_Higher_Than_80\n",
    "FROM\n",
    "  walmart_stock\n",
    "WHERE\n",
    "  Close < 80;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794f312d-560d-4c84-a504-fdfa46ff1157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Date', 'Min_of_volume', 'Max_of_volume', 'Low', 'Close', 'Volume',\n",
       "       'Adj Close'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099d0b23-12f5-4786-bb61-2654f3a91fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1655643824988898>, line 11\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m df1 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/shared_uploads/suriyakrishnan4798@gmail.com/walmart_stock-1.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Create a temporary view for Spark SQL\u001B[39;00m\n",
       "\u001B[0;32m---> 11\u001B[0m df\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwalmart_stock\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Write a Spark SQL query\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m sql_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124m    SELECT COUNT(*) AS OpenStockCount\u001B[39m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;124m    FROM walmart_stock\u001B[39m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;124m    WHERE OPEN = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWalmart\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/generic.py:5575\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   5568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   5569\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n",
       "\u001B[1;32m   5570\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n",
       "\u001B[1;32m   5571\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n",
       "\u001B[1;32m   5572\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n",
       "\u001B[1;32m   5573\u001B[0m ):\n",
       "\u001B[1;32m   5574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n",
       "\u001B[0;32m-> 5575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'createOrReplaceTempView'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1655643824988898>, line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m df1 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/shared_uploads/suriyakrishnan4798@gmail.com/walmart_stock-1.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Create a temporary view for Spark SQL\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m df\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwalmart_stock\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Write a Spark SQL query\u001B[39;00m\n\u001B[1;32m     14\u001B[0m sql_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124m    SELECT COUNT(*) AS OpenStockCount\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124m    FROM walmart_stock\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m    WHERE OPEN = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWalmart\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/generic.py:5575\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   5568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   5569\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[1;32m   5570\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[1;32m   5571\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[1;32m   5572\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[1;32m   5573\u001B[0m ):\n\u001B[1;32m   5574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[0;32m-> 5575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'createOrReplaceTempView'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'createOrReplaceTempView'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WalmartStockStreaming\").getOrCreate()\n",
    "\n",
    "# Read data from Excel\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/suriyakrishnan4798@gmail.com/walmart_stock-1.csv\")\n",
    "\n",
    "# Create a temporary view for Spark SQL\n",
    "df.createOrReplaceTempView(\"walmart_stock\")\n",
    "\n",
    "# Write a Spark SQL query\n",
    "sql_query = \"\"\"\n",
    "    SELECT COUNT(*) AS OpenStockCount\n",
    "    FROM walmart_stock\n",
    "    WHERE OPEN = 'Walmart'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result = spark.sql(sql_query)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e51198-46d9-41e9-aa2e-f4ab274e5f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-2002338860585168>, line 1\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    git commit -m\u001B[0m\n",
       "\u001B[0m        ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-2002338860585168>, line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    git commit -m\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (command-2002338860585168-436022534, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks_Task",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
